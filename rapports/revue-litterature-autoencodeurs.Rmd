---
title: "Litterature review - Autoencoders"
subtitle: "Département de mathématiques et de statistique - Université Laval"
author: "Stéphane Caron"
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes:
- \usepackage{float} #use the 'float' package
- \floatplacement{figure}{H} #make every figure with caption = h
output: 
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    number_sections: true
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos= "h")
```

# Introduction to autoencoders

An autoencoder is an unsupervised neural network technique that aims to learn an efficient intermediate representation of an input [@Goodfellow-et-al-2016]. To achieve this objective, the autoencoder has 2 components: an **encoder** and a **decoder**. The encoder receives an input $x$ and and converts it to a hidden representation $z$. The decoder receives a representation $z$ and decodes it back to retrieve as much as possible the input $x$. Historically, autoencoders were known as a dimensionnalty reduction method, but it has now more applications by learning latent variables useful in generative modelling and other fields.

## Architecture

As explained in the introduction, autoencoders are divided in two parts: the encoder and the decoder. The encoder transforms the information into a given representation and the decoder reconstructs it back to its original format (see figure \@ref(fig:ex1)).

```{r ex1, fig.cap="Autoencoders structure example.", out.width = '60%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/autoencoder-structure.png")
```

In most cases, the hidden representation (or latent representation) is smaller because of the structure of the layers between the input and that representation. Autoencoders that have a smaller hidden representation than its inputs are called *undercomplete*. In those cases, the constraints on $z$ force the network to learn the more important features. It could be applied to feedforward networks or to convolutional networks. We are typically interested in that hidden representation, while the decoder is more often than not discarded.


## Optimization

Autoencoders intuition is to build back the input $x$ by passing through the 2 components (encoder and decoder). As such, this kind of model does not need any target, so we say it is an unsupervised method. The training of the parameters is mainly done by minimizing the reconstruction error. The loss is given by:

$$
L(x, p_\phi{\{q_\theta(x)\}})
$$

where $q(x, \theta)$ is the encoder and $p(z,\phi)$ is the decoder function. The loss generally includes another component $\Omega(x)$ that acts as a regularizer. In that case, the loss is now given by:

$$
L(x, p_\phi{\{q_\theta(x)\}}) + \Omega(x)
$$

The regularization is often essential to prevent the network to "copy/paste" examples of the training set. There are multiple ways to introduce regularization in the model. In fact, it can be directly on the input, on the parameters to learn or even on the latent representation (we'll cover different types of regularization applied to autoencoders in the next sections). Overfitting situations can occur even more when the hidden representation is equal or greater than the inputs (*overcomplete*). The minimization of the loss function is done by gradient descent. For instance, the encoder and decoder parameters are gradually updated by:

\begin{equation} \label{optim}
\Theta \leftarrow \Theta-\epsilon*\frac{\partial L}{\partial\Theta}
\end{equation}

where $\epsilon$ is a learning weight that aims to control the size of the learning steps and $\Theta : \{\theta, \phi\}$ includes both encoder and decoder parameters.

## Example

Let's illustrate the high-level concepts presented in the last 2 sections with an overly simplistic example. We have a input $X$ with $p=2$ features and 10 observations. We want to encode those 2 features in one single features using an basic autoencoder with 1 hidden layer. In summary, our autoencoder has 3 layers (input layer, hidden layer and output layer). We also choose a sigmoid activation function for our hidden layer and a linear activation function for our ouput layer (those choices are arbitrary).

```{r ex2, fig.cap="Autoencoders structure example.", out.width = '60%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/example-toy-ae.pdf")
```

The encoder function (that transforms $x$ to $z$) is expressed as:

$$
q_{\theta}(x)=h\big(x_1*w_{1,1} + x_2*w_{1,2} + b_1\big)
$$

where $h(x)$ is a sigmoid function in the form of $h(x)=\frac{1}{1+e^{-x}}$. The parameters $\theta$ that need to be optimized are $w_{1,1}, w_{1,2}$ and $b_1$. We can also use the matrix notation :

$$
q_{\theta}(X)=h\big(X*\boldsymbol w_1 + b_1\big)
$$

where $X = [\boldsymbol x_1, \boldsymbol x_2]$. For one single observation, we could defined $X^{(i)} = [x_{1}^{(i)}, x_{2}^{(i)}]$. The vector of weights $\boldsymbol w_1$ is defined as $\boldsymbol w_1 = [w_{1,1}, w_{1,2}]$.

In the same fashion, the decoder is expressed as:

\begin{align*}
  p_{\phi}(Z)=
  \begin{cases}
    p_{\phi}^1(z) = z*w_{2,1}+b_2 \\
    p_{\phi}^2(z) = z*w_{2,2}+b_2
  \end{cases}
\end{align*}

The parameters $\phi$ that need to be optimized are $w_{2,1}, w_{2,2}$ and $b_2$. We can also use the matrix notation (which is more convinient in that case):

\begin{gather*}
  p_{\phi}(Z)=h\big(Z*\boldsymbol w_2 + b_2\big)=\hat{X} \\
  p_{\phi}: \rm I\!R \rightarrow \rm I\!R^2
\end{gather*}

where $\boldsymbol w_2 = [w_{2,1}, w_{2,2}]$. 

To optimize those parameters $\theta$ and $\phi$, we need to define a loss function (see equation \@ref(perte1)). Let's say we want to minimize the reconstruction error only, we can define our loss as:

\begin{equation} \label{perte1}
\begin{split}
L(x,\hat{x}) & = L\big(x, p_\phi\{q_\theta(x)\}\big) \\
& = \frac{1}{2} \sum_{i=1}^{n} [x^{(i)}-p_\phi\{q_\theta(x^{(i)}\}]^{\text{T}}[x^{(i)}-p_\phi\{q_\theta(x^{(i)}\}] \\
& = \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{p} \big(x_{j}^{(i)}-p^j_\phi\{q_\theta(x_{j}^{(i)})\}\big)^2
\end{split}
\end{equation}

where $i$ stands for the $i$-th observation and $j$ for the $j$-th element of the input $X$.

Once we have a loss function, we can optimize the parameters of the encoder/decoder using the *chain rule* of derivatives for each layer (see equation \@ref(optim)).


# Types of autoencoders

In the previous sections, we introduced the general idea behind the autoencoders. However, there are several types of autoencoders and each has it own strengths and weakenesses. They differ in their architecture and optimization process, but in the end, they all consist in unsupervised methods that learn latent structure of the data.

## Origins of autoencoders

The origins of autoencoders were known in the 80s and introduced by [@Rumelhart-1986]. In their work, they essentially learn a hidden representation using the input as the output of the model.

## Deep autoencoders

With the resurgence of neural network and the introduction of deep neural network at the beginning of 2000's, a new generation of autoencoders with multiples hidden layers were introduced as "deep autoencoders", by [@HintonSalakhutdinov2006b]. In the following years, many variants have been applied to autoencoders, mainly regarding the regularization component, to be able to better generalize useful representation and structure of the data.  

## Sparse autoencoders

One way of learning useful representations using autoencoder is to impose a constraint of **sparsity** in the hidden representation [@ranzato-2007]. A sparse autoencoder is basically an autoencoder that has a sparsity component in the loss criterion:

$$
L(x, p_\phi\{q_\theta(x)\}) + \Omega(q_\theta(x))
$$

where $z$ is the hidden representation (or the output of the encoder). That sparsity component ($\Omega(z)$) will pull some neurons output values towards zero in the hidden layer, making them "inactive" [@Ng-2011]. That regularization will prevent overfitting and prevent the network to learn by heart every input. We can relate that idea with the concept of dropout, where some neurons are randomly ignored during the training process. The global idea of those 2 concepts is to prevent overfitting by *canceling* the effects of some neurons in the hidden representation (sparsity component) or in different layers (dropout). However, the difference is that dropout adds noise to the training by randomly forcing some neuron outputs to zero while the sparsity component pull towards zero the weights of neurons having minimal impact on training.

Sparse autoencoders are generally used to learn features for another task (e.g classification). In that sense, it can accomplish the feature engineering of a task by learning useful features.

## Denoising autoencoders

Another kind of autoencoder that has been widely used are the denoising autocoders [@Vincent-2008]. These autoencoders are really close to [deep autoencoders"](#deep-autoencoders) except they receive a corrupted input and are trained to produce an uncorrupted output.

The denoising autoencoders will then minimize:

$$
L(x, p_\phi\{q_\theta(\tilde{x} | x)\}) 
$$
where $\tilde{x}$ is a copy of $x$ that has been corrupted.

There are different ways of corrupting the input [@journals/jmlr/VincentLLBM10]. One way is to choose a given proportion $\nu$ of inputs to randomly "destruct" by assigning a 0-value while keeping the other inputs untouched. That approach is called *masking noise*. The input can also be corrupted by adding *Gaussian noise* or setting randomly a proportion of the inputs to maximum or minimum value (*salt-and-pepper noise)*. The intuition behind this approach is again to prevent the algorithm from learning the useless identity function.

## Contractive autoencoders

The contractive autoencoders [@Rifai-2011] are often associated with the denoising autoencoders. The loss criterion includes a regularizer penalty in the form of:

$$
\Omega = \lambda \Bigg| \Bigg| \frac{\partial q_\theta(x)}{\partial x}\Bigg| \Bigg|^{2}_F{}
$$

where $F$ stands for Frobenius norm (sum of squared elements). In this case, we are adding a penalty corresponding to the Jacobian matrix of the partial derivatives of the encoder function. 

Denoising autoencoders are built to make the reconstruction function resist small but finite-size perturbations of the inputs, while contractive autoencoders make the feature extraction function (the encoder) resist small changes in the inputs.

## Variational autoencoders

The variational autoencoders [@kingma2013autoencoding] have a slightly different approach than other kinds of autoencoders and are particularly useful in generative modelling and reinforcement learning. Again, these autoencoders have a loss criterion in the form of:

$$
L = \text{reconstruction error} + \text{regularizer}
$$

However, instead of encoding a hidden representation of size $n$, it outputs two vectors of size $n$: a vector of means $\boldsymbol \mu$ and a vector of standard deviations $\boldsymbol \sigma$. Those vectors allow to sample from Gaussian distributions, which gives the variational autoencoders the unique property of having a latent space that is **continuous** [@toward-VAE]. This is the main difference with previous seen autoencoders. In other words, the basic autoencoders learn a reprensetation that "points" somewhere in the latent space, while varitional autoencoders learn a representation that points to an "area", an area that is defined by the mean $\boldsymbol \mu$ and the standard deviation $\boldsymbol \sigma$ of the latent space. That property is actually very useful in generative modelling. The figure \@ref(fig:vae) illustrates the basic structure of a variational autoencoder.

```{r vae, fig.cap="Variational autoencoder structure. Instead of learning a representation directly, it learns the parameters of Gaussian distributions and the hidden representation is given by sampling from those distributions.", out.width = '60%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/vae-structure.png")
```

The explicit variational autoencoder loss criterion is given by :

$$
L(x, p_\phi\{q_\theta(x)\}) + D_{KL}\big[q_\theta(z|x) || p(z)\big]
$$

where $D_{KL}$ is a regularizer called the Kullback-Leibler divergence. Its goal is to ensure that the two distributions $p(z)$ and the encoder ($q_{\theta}(x)$) are somewhat similar. The function $p(z)$ is a Gaussian distribution $N(0, I)$. 

## Adversarial autoencoders

Adversarial autoencoders [@Makhzani-2015] are really close to the variational autoencoders in the sense that they also produce a continuous latent space by learning a certain distribution. However, this prior distribution is set in advance and learned in an adversarial fashion instead of being learned by a divergence crtierion. That's mainly why we say that adversarial autoencoders are the combination of variational entoencoders and generative adversarial networks (GAN). The figure \@ref(fig:aae) shows the architecture of an adversarial autoencoder. In that figure, the upper portion corresponds to the standard autoencoder that reconstruct the input $x$ from a latent representation $z$. The bottom portion is a second network trained to predict whether a sample arise from the hidden representation or from a distrubution decided in advance. As such, the autoencoder will try to learn the prior distribution and fool the model at the bottom. That kind of approach is often related with generative modeling.

```{r aae, fig.cap="Basic architecture of a adversarial autoencoder.", out.width = '100%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/adversarial-structure.png")
```

Like other autoencoders, adversarial autoencoders learn an encoding distribution $q_{\theta}(z|x)$. In order to do it, we impose a certain distribution on the hidden representation that we express as $p(z)$. If we express the data distribution as $p_d(x)$, we can express the posterior distribution of $z$ as :

$$
q(z) = \int_{x}q_{\theta}(z|x)p_d(x)dx
$$

In the training process, the adversarial autoencoder is trying to match the $q(z)$ and the $p(z)$ distributions. There are several choices for the prior distribution from which to learn:

- Gaussian distribution
- Gamma distribution
- etc

Adversarial autoencoders are generative models that could be used to generate new data, or also to do semi-supervised learning or clustering.

# One-class classification

In one-class classification problems, one class (which is usually refered as positive or target class in the litterature) is well characterized by instances in the training dataset while the other classes (negatives or outliers) have either no instances or very few of them [@DBLP:journals/corr/KhanM13]. In in nutshell, the objective is generally to build a subspace that contains as much positive examples as possible and excludes the negative ones, without knowing the targets. This task is often used to identify or remove small number of anomalies or outliers within a dataset. We can distinguish 3 broad categories: density estimation, boundary methods and reconstruction methods.

## Density estimation

Gaussian and mixture of Gaussians are 2 well known density estimation methods. These methods assume the data is distributed according to the normal distribution or to a mixture of normals. The parameters of these normal distributions can be found iteratively with EM algorithm. As soon as the density are estimated, we can use a certain threshold to determine what should be considered as a positive or inlier observations.

Other density estimation alternatives could be the Parzen and Naive Parzen methods. Those nonparametric methods do not make any assumptions about the data distribution. In fact, those methods do not need any parameters estimations. 

## Boundary methods

A well known boundary method is the one-class SVM or more generally the Support Vector data description method (SVDD). In this case, we want to define a hypersphere with minimal volume covering the majority of probability mass of the data. The number of parameters to be estimated is equal to the size of the training dataset, which makes this method less effective when dealing with large dataset. One usefull feature of one-class SVM is that we can leverage kernel functions, which allow us to represent complexe subspaces.

## Reconstruction methods

Reconstruction methods are generally associated with matrix factorization, which has become popular in the recent years. In brief, these methods provide a reconstruction matrix $\hat{X}$ of the input $X$ and use a certain norm measure to determine which observations of $X$ are anomalous. 

Principal component analysis (PCA) could be considered as a reconstruction anomaly detection method. This method basically finds the directions that maximizes variances in the data. The projection of a matrix $X_{n \times p}$ into a lower dimensions matrix $T_{n \times d}$ (where $d < p$) could be expressed as:

$$
T_d=XW_d
$$
where $W_d$ is the $d$ first columns of the eigenvectors matrix of $X^TX$. The reconstructed matrix $\hat{X_d}$ could then be retrieved with $T_d W_{d}^{T}$. To identify anomalous data, we could look at observations of this reconstructed matrix that are far from the initial $X$ matrix. However, PCA is highly sensitive to data perturbations, as one anomaly can increase a lot the variance within one component and completely changes the projection directions which can often hide anomalies.

To tackle the problem just mentionned, other versions of PCA were developped such as Robust PCA. This method use a more clever way of decomposing the data to isolate the noise. However, this method is still doing a linear decomposition of the data.

# Applications of autoencoders

This section aims to survey the different known applications of autoencoders in the litterature and across different industries.

## Dimensionality reduction

One of the first applications for which the autoencoders were used was associated with **dimensionality reduction**. For example, [@HintonSalakhutdinov2006b] showed that a **deep autoencoder** could learn to represent data in a smaller representation better than the widely used PCA method (see figure \@ref(fig:pca)).

```{r pca, fig.cap="Comparison between the learned representation of a PCA and a deep autoencoder.", out.width = '100%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/pca-vs-ae.png")
```

## Information retrieval

Another application of autoencoders is related with **information retrieval**. This is the task of finding an entry similar to another entry in a database. It can be done by dimensionnality reduction, and even binarization of an input, which is called semantic hashing. This has been applied to both text [@Salakhutdinov-2009] and images [@Weiss2008SpectralH].

## Anomaly detection

A task that is often related with unsupervised methods is the idea of finding outliers or anomalies in a dataset. In fact, that's normally an information we don't have and that we can't train on it. Thus, we generally need unsupervised ways of discovering those anomalies. Anomalies can often be interesting from a business perspective when we consider they could be related with examples such as: frauds, medical problems, structural defects, malfunctioning equipment, etc. 

For instance, [@zhou-2017] used **deep autoencoders** to build an unsupervised anomaly detection algorithm called *robust deep autoencoders*. They applied their methodology, that includes an anomaly regularizing penalty, on MNIST dataset to find anomalies in hand written digits. Their intuition is that they can isolate the noise and outliers in the input, and the autoencoder is trained after this isolation. Their model is inspired by Robust Principal Component Analysis where the input $X$ is decomposed in 2 parts: $L_D + S$, where $L_D$ can be effectively reconstructed by an autoencoder and $S$ contains the noise and the outliers in the original data. 

Another interesting example of anomaly detection using autoencoders was made by [@zong2018deep]. They used **deep autoencoders** to build a small representation and use that representation as well as the reconstruction error to train a Gaussian Mixture Model (GMM). The figure \@ref(fig:DAGMM) shows the overview of the compression (deep autoencoder) and estimation (GMM) tasks.

```{r DAGMM, fig.cap="Overview of the Deep Autoencoder and Gaussian Mixture Model.", out.width = '100%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/dagmm.png")
```

In the figure above, the estimation network estimates the GMM parameters without using EM algorithm. 

Autoencoders and density estimation are often used togheter in anomaly detection problems. It is the case in one-class learning problem such as [@Cao-2016]. Their work showed that it's possible to use the power of **deep autoencoders** to learn a small compact representation and then use density estimation techniques such as Centroid or KDD that work best in low-dimensionnal situations.

In [@DBLP:journals/corr/abs-1802-06360] the authors proposed an anomaly detection framework where the representation learned by the autoencoder is oriented to the task of detecting anomaly. In fact, they first train an autoencoder and then reuse the encoder part to output a compressed representation of the input that will be used as input for a one-class neural network. The encoder parameters are then adjusted based on the error made by the one-class neural network. This one-class model is a basic feed-forward network that replicates the optimization of a one-class SVM. In a nutshell, they leverage the capacity of deep neural networks to extract rich representation of the data along with a one-class objective.

## Clustering

A well known task in the unsupervised world is the task of clustering. [@DilokthanakulMG16] used **varitional autoencoders** and introduced a Gaussian Mixture Model (GMM) as a prior distribution with the goal of performing clustering. They also discuss the problem of over-regularisation than varitional autoencoders can have and how to tackle this problem.

[@Xie-2016] also proposed a deep embedding approach that simultaneously learns feature reprensentations and cluster assignements. Their methodology consists of first initializing a **denoising autoencoder**. Once that first model is trained, they are able to represent the data is a smaller representation (they drop the decoder). They also initialize clusters assignements by using this representation and the $k$-means algorithm. Then, they simultanuasly train the $k$ cluster centers in the feature space $Z$ and the parameters $\theta$ that maps data points to $Z$. The figure \@ref(fig:xie2016) illustrates the network structure where we can see the autoencoder at the top (the learned encoder in gray dashed rectangle), and then the *deep embedded clustering* network at the bottom.

```{r xie2016, fig.cap="Deep Embedded Clustering network strcuture", out.width = '100%', fig.align='center', fig.pos=""}
knitr::include_graphics("images/xie-2016.png")
```

[@Xi-2018] also used autoencoders to make clustering on a complex space where the non-linearity of neural networks allowed them to learn complex subspaces.

Multivariate density estimation is widely performed for fundamental unsupervised tasks such a clustering. [@ijcai2018-374] used **variational autoencoders** and applied a Student-$t$ distribution instead of a Gaussian distribution for the decoder.

\newpage

# References
