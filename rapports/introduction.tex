\chapter*{Introduction}         % ne pas numéroter
\label{chap:introduction}       % étiquette pour renvois
\phantomsection\addcontentsline{toc}{chapter}{\nameref{chap:introduction}} % inclure dans TdM


La détection d'anomalies est un sujet complexe qui a généré beaucoup de littérature en statistique,  en apprentissage machine et plus récemment en vision numérique. Il existe plusieurs applications de ces méthodes dans les domaines de la cyber-intrusion, de la finance et de l'assurance, de la médecine ou dans l'identification de dommages industriels \citep{chandola2009anomaly}. Une anomalie est définie par \cite{Zimek2017} comme un événement, une mesure ou une observation qui diffère significativement de la majorité des données. Une anomalie, également appelée une aberration, est une notion intrinsèque à plusieurs domaines reliés à l'analyse des données, car une donnée anormale ou aberrante est généralement intéressante à identifier ou à retirer d'une source de données. Dans un premier temps, il peut être intéressant de l'identifier simplement parce que c'est  l'objectif poursuivi en soi. Dans un deuxième temps, il peut également être pertinent de retirer une observation anormale ou aberrante avant de réaliser une autre tâche d'apprentissage. 

Une difficulté reliée à la détection d'anomalies est qu'on doit souvent utiliser des données non étiquetées, car les anomalies sont généralement générées par des phénomènes imprévus. Cela fait en sorte qu'on doit approcher le problème de manière non-supervisée. Une autre difficulté, plus particulièrement reliée avec les approches non-supervisées, est qu'il faut généralement déterminer un seuil nous permettant de prendre une décision quant à la nature d'une donnée (anomalie ou non). Finalement, ces difficultés deviennent encore plus prononcées lorsqu'on doit traiter des données complexes, comme des images.

Dans le contexte de données non structurées, comme des images, du texte ou du son, les réseaux de neurones sont couramment utilisés. En effet, leurs couches superposées peuvent traiter une entrée complexe, comme une image, et compresser cette information vers une représentation vectorielle plus compacte et riche en informations. Récemment, plusieurs approches d'apprentissage profond ont été proposées dans l'objectif de détecter un changement de distribution. C'est d'ailleurs un problème d'éthique potentiel qui a été rapporté par \cite{amodei2016concrete}. Ces approches de \textit{out-of-distribution detection} sont utilisées pour détecter des images qui semblent "anormales" \citep{ren2019likelihood}. Ces méthodes ne seront cependant pas étudiées dans ce mémoire. L'accent est plutôt mis sur les autoencodeurs, des approches non-supervisées qui sont utilisées dans un contexte de détection d'anomalies. Les autoencodeurs sont effectivement une catégorie de réseaux de neurones fréquemment utilisés en apprentissage non-supervisé. Il existe d'ailleurs plusieurs applications d'autoencodeurs dans un contexte de détection d'anomalies, où l'erreur de reconstruction est souvent utilisée comme indicateur d'anomalie. Cependant, ces méthodes requièrent de fixer un seuil de détection, souvent sous forme de distance ou de métrique, qui peut être difficile à établir ou à expliquer. C'est d'ailleurs pour cette raison que  \cite{An2015VariationalAB} proposent de se baser sur une probabilité de reconstruction, qui est une mesure objective et ne requiert pas de seuil quelconque. Par contre, cette mesure de probabilité est basée sur la capacité de reconstruire les observations originales, ce qui peut être problématique dans le contexte d'images complexes, plutôt que sur la représentation latente, qui elle peut être beaucoup plus simple. D'autres approches, comme dans \cite{chalapathy2018anomaly}, se basent plutôt sur cette représentation latente pour faire la détection d'anomalies. C'est davantage vers ce type d'approches que nous souhaitons orienter notre méthodologie. 

Dans ce mémoire, nous souhaitons contribué à cette partie de la littérature, soit la détection d'anomalies dans un jeu de données d'images, en utilisant un score d'anomalie facilement interprétable. Nous proposons également de simplifier la détermination du seuil nécessaire pour conclure quelles observations sont "normales" et quelles observations sont "anormales". Avec cette contribution, nous proposons d'utiliser des méthodes existantes comme des autoencodeurs  pour encoder des structures de données complexes dans des représentations latentes. Ces représentations latentes et leur erreur de représentation serviront ensuite de base pour un cadre décisionnel simple et intuitif à appliquer. Nous illustrons ensuite comment, à partir d'autoencodeurs variationnels (VAE), nous sommes en mesure de créer ces dites représentations et de détecter les anomalies dans un ensemble d'images.