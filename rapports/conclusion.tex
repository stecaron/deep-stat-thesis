\chapter*{Conclusion}           % ne pas numéroter
\label{chap:conclusion}         % étiquette pour renvois
\phantomsection\addcontentsline{toc}{chapter}{\nameref{chap:conclusion}} % inclure dans TdM

La détection d'anomalies est une tâche qui comporte généralement plusieurs défis en modélisation et en analyse de données. Parmi ces défis, on peut mentionner le débalancement entre la classe "normale" et "anormale". Ce débalancement peut s'expliquer par le fait qu'une anomalie est par définition un événement qui diffère significativement de la majorité des données \citep{Zimek2017} et ainsi peut probable. Ensuite, la détection d'anomalies nous amène généralement dans un contexte d'apprentissage non-supervisé. En effet, il est rare qu'on possède des étiquettes nous indiquant si une observation est "anormale" ou pas. Un défi important relié à ce contexte d'apprentissage non-supervisé est que l'analyste ou le chercheur se retrouve souvent dans une situation où il doit déterminer un seuil quelconque afin de pouvoir être en mesure discriminer les observations "normales" des observations "anormales". L'établissement de ce seuil est généralement basé sur une mesure de distance qui peut être difficile à établir dans certains cas et peut également différer d'une expérience à une autre. Ces défis reliés à la détection d'anomalies deviennent encore plus importants lorsqu'on doit traiter des données complexes, comme par exemple des images. Cette complexité supplémentaire peut se traduire par une forte dimensionnalité des données d'entrées et aussi par une notion de dépendance et de localité entre les pixels voisins d'une image.  Dans ce mémoire, nous démontrons la pertinence d'utiliser des autoencodeurs variationnels pour adresser les défis traditionnelles reliés à la détection d'anomalies tout en restant efficace dans une contexte de données complexes comme des images. En bout de ligne, notre approche permet d'identifier les images d'un jeu de données qui dévient de la normalité par rapport à un niveau de filtration, ou d'acceptabilité, intuitif et simple à établir.

Dans les expérimentations réalisées et présentées dans le chapitre \ref{chap:experiments}, nous sommes en mesure de conclure que notre approche produit des résultats performants autant sur des jeux de données d'images réelles que d'images simples. La valeur ajoutée de notre approche par rapport à d'autres méthodes est toutefois beaucoup plus évidente dans le cas d'images réelles. En effet, les performances obtenues par notre approche sur des images simples, comme \textit{MNIST}, ne sont généralement pas significativement meilleures ou pires que d'autres approches. En revanche, les performances en aire sous la courbe ROC, en précision et en rappel obtenues sur des images réelles plus complexes, comme \textit{ImageNet}, sont supérieures aux autres approches comparatives. Nous avons remarqué que l'autoencodeur variationnel permet de représenter les données complexes dans une forme latente beaucoup plus simple et qui permet aussi de bien discriminer les anomalies des observations "normales". 

Notre approche pourrait être utilisée pour plusieurs applications ou situations réelles où l'on cherche à identifier des images "anormales". Prenons l'exemple où l'on bénéficie d'un ensemble d'images propres pour lequel on sait qu'il n'y a pas d'anomalies ou de défectuosités. Supposons que dans le futur, on prévoit recevoir de nouvelles images qui sont supposées représenter la même chose que notre ensemble d'images propres, mais dont certaines de ces nouvelles images sont défectueuses où représentent quelque chose de complètement différent. Nous ne connaissons pas d'avance à quoi ces images défectueuses peuvent ressembler et n'avons aucun exemple d'image de la sorte sur lesquelles on pourrait faire un apprentissage. Notre approche permettrait donc de déterminer lesquelles de ces nouvelles images sont potentiellement défectueuses ou ne semblent pas représenter la même chose que notre ensemble d'images a priori. On pourrait également penser à des exemples d'applications où l'on cherche à nettoyer un jeu de données d'images. En effet, on pourrait valider et confirmer un sous-ensemble d'images propres qui représentent le jeu de données que l'on souhaite avoir. Ensuite, on pourrait utiliser ce sous-ensemble comme jeu de données d'entraînement pour notre approche. Finalement, on pourrait utiliser le reste du jeu de données initial comme jeu de données de test et ainsi valider si des images "anormales" font parties de notre ensemble global. Cela reviendrait donc utiliser notre approche comme technique de nettoyage de données sans avoir à valider toutes les images manuellement.

Dans le futur, il serait intéressant de voir si des variations de l'autoencodeur variationnel pourraient donner des résultats encore plus performants en détection d'anomalies. Parmi les possibilités de variations, il serait intéressant de voir si une autre loi que la $N(0,I)$ pourrait être utilisée comme loi a priori de la représentation latente et voir si cela permettrait d'obtenir une représentation latente qui discrimine encore mieux les anomalies des observations "normales". Ensuite, il serait également intéressant de voir s'il est possible de faire en sorte que les représentations latentes des anomalies se comportent toujours de la même manière par rapport à un point de référence, soit la distribution $N(0,I)$ dans notre cas. En effet, nous avons remarqué que selon la nature des données, les représentations latentes des anomalies se retrouvaient dans certains cas plus près de la $N(0,I)$ et dans certains cas  plus éloignées de la $N(0,I)$. Ce constat ajoute une étape manuelle pour mettre en place notre méthodologie, ce qui rend l'approche moins autonome et peut-être plus difficile à appliquer dans la pratique. Au final, nous pouvons conclure que l'autoencodeur variationnel possède des propriétés intéressantes, dont l'apprentissage d'une représentation latente utile, pour faire de la détection d'anomalies sur des données complexes comme des images.


